{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62dbe7c",
   "metadata": {},
   "source": [
    "# What Is a T-Test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7cbe4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f6865c3",
   "metadata": {},
   "source": [
    "- A t-test is an inferential statistic used to determine if there is a statistically significant difference between the means of two variables.\n",
    "- The t-test is a test used for hypothesis testing in statistics.\n",
    "- Calculating a t-test requires three fundamental data values including the difference between the mean values from each data set, the standard deviation of each group, and the number of data values.\n",
    "- T-tests can be dependent or independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e09f9",
   "metadata": {},
   "source": [
    "https://www.wallstreetmojo.com/t-test/#t-test-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c4efd",
   "metadata": {},
   "source": [
    "## 5 types of T-Test\n",
    "\n",
    "#### 1. One-Sample T-Test : One-sample is used to determine whether an unknown population mean is different from a specific value.\n",
    "Ex: Null hypothesis : The average height of Vietnamese men is 1m70. Alternative hypothesis(H1): The average height of Vietnamese men is 1m75\n",
    "#### 2. Independent Two-Sample T-Test : An independent Two-Sample test is conducted when samples from two different groups, species, or populations are studied and compared.\n",
    "Ex: One way to measure a person’s fitness is to measure their body fat percentage. Average body fat percentages vary by age, but according to some guidelines, the normal range for men is 15-20% body fat, and the normal range for women is 20-25% body fat.\n",
    "#### 3. Paired Sample T-Test: Paired Sample is the hypothesis testing conducted when two groups belong to the same population or group.\n",
    "A paired samples t-test is commonly used in two scenarios:\n",
    "- A measurement is taken on a subject before and after some treatment – e.g. the max vertical jump of college basketball players is measured before and after participating in a training program.\n",
    "- A measurement is taken under two different conditions – e.g. the response time of a patient is measured on two different drugs.\n",
    "#### 4. Equal Variance T-Test : Equal Variance is conducted when the sample size in each group or population is the same, or the variance of the two data sets is similar.\n",
    "Two-sample T-Test with equal variance can be applied when \n",
    "- the samples are normally distributed, \n",
    "- the standard deviation of both populations are unknown and assumed to be equal, and \n",
    "- the sample is sufficiently large (over 30). \n",
    "Ex : To compare the height of two male populations from the United States and Sweden, a sample of 30 males from each country is randomly selected and the measured heights are provided.\n",
    "#### 5. Unequal Variance T-Test : Unequal Variance is used when the variance and the number of samples in each group are different.\n",
    "Two-sample T-Test with unequal variance can be applied when \n",
    "- the samples are normally distributed, \n",
    "- the standard deviation of both populations are unknown and assume to be unequal, \n",
    "- sample is sufficiently large (over 30). \n",
    "Ex : To compare the height of two male populations from the United States and Sweden, a sample of 30 males from each country is randomly selected and the measured heights are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2493d4",
   "metadata": {},
   "source": [
    "# What Is a p-value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c71359",
   "metadata": {},
   "source": [
    "Xem Goodnote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef941e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "######################################## Data preparation #########################################\n",
    "\n",
    "file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "X = df[['Por', 'VR']].values.reshape(-1,2)\n",
    "Y = df['Prod']\n",
    "\n",
    "######################## Prepare model data point for visualization ###############################\n",
    "\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = Y\n",
    "\n",
    "x_pred = np.linspace(6, 24, 30)      # range of porosity values\n",
    "y_pred = np.linspace(0.93, 2.9, 30)  # range of VR values\n",
    "xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\n",
    "model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, Y)\n",
    "predicted = model.predict(model_viz)\n",
    "\n",
    "############################################## Evaluate ############################################\n",
    "\n",
    "r2 = model.score(X, Y)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel('Porosity (%)', fontsize=12)\n",
    "    ax.set_ylabel('VR', fontsize=12)\n",
    "    ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=27, azim=112)\n",
    "ax2.view_init(elev=16, azim=-51)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.suptitle('$R^2 = %.2f$' % r2, fontsize=20)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a697d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed349440",
   "metadata": {},
   "source": [
    "# Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2701d7",
   "metadata": {},
   "source": [
    "Measure the linear association between two numerical variables(if one go up, another should go up, if one go down, another should go down). Apply in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e2530",
   "metadata": {},
   "source": [
    "# Kendall Correlation Coefficient(Kendall's Tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55289223",
   "metadata": {},
   "source": [
    "One less commonly used than Pearson Correlation is Kendall’s Tau, which measures the relationship between two columns of ranked data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9845e",
   "metadata": {},
   "source": [
    "https://www.statology.org/kendalls-tau/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c34d1",
   "metadata": {},
   "source": [
    "# Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a2881",
   "metadata": {},
   "source": [
    "A Chi-Square goodness of fit test is used to determine whether or not a categorical variable follows a hypothesized distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe23ee",
   "metadata": {},
   "source": [
    "https://www.statology.org/chi-square-test-by-hand/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4907630",
   "metadata": {},
   "source": [
    "# Correlation Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5188ccc",
   "metadata": {},
   "source": [
    "A correlation matrix is simply a table which displays the correlation coefficients for different variables. The matrix depicts the correlation between all the possible pairs of values in a table. It is a powerful tool to summarize a large dataset and to identify and visualize patterns in the given data.M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d53643",
   "metadata": {},
   "source": [
    "There are three broad reasons for computing a correlation matrix:\n",
    "\n",
    "- To summarize a large amount of data where the goal is to see patterns. In our example above, the observable pattern is that all the variables highly correlate with each other.\n",
    "- To input into other analyses. For example, people commonly use correlation matrixes as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\n",
    "- As a diagnostic when checking other analyses. Correlation matrix can prevent mis-interpretations and errors linked to correlated variables input in the models. For example, with linear regression, a high amount of correlations suggests that the linear regression estimates will be unreliable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97a6ae",
   "metadata": {},
   "source": [
    "https://www.displayr.com/what-is-a-correlation-matrix/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704c7b2",
   "metadata": {},
   "source": [
    "# 4 Categorical Encoding Concepts to Know for Data Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fd732",
   "metadata": {},
   "source": [
    "- One hot encoding\n",
    "- Label encoding\n",
    "- Hash encoding \n",
    "- Target encoding\n",
    "- Leave One Out Encoding (LOOE) (Same as target encoding but add Gaussian noise distribution to decrease overfitting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8962f",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64\n",
    "\n",
    "https://towardsdatascience.com/4-categorical-encoding-concepts-to-know-for-data-scientists-e144851c6383"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d187a",
   "metadata": {},
   "source": [
    "# None linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a04959",
   "metadata": {},
   "source": [
    "- Quadratic Relationships\n",
    "- Cubic Relationships\n",
    "- Exponential Relationships\n",
    "- Logarithmic Relationships\n",
    "- Cosine relationships\n",
    "\n",
    "https://www.statology.org/nonlinear-relationship-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517aefb",
   "metadata": {},
   "source": [
    "# Machine learning model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e421cb",
   "metadata": {},
   "source": [
    "Motivation:\n",
    "\n",
    "There are some models  like linear models, logistic regression and tree-bases can be easily interpreted because of their intuitive way of getting to the predictions. On the other hand, you have more complex models like ensemble models (e.g., Random Forest, XGBoost, etc.) and deep neural networks, which are especially hard to interpret because of their complexity.  For real-world problems like fraud detection, self-driving cars, or loan lending, the model doesn't only need to perform well but also needs to be easily interpretable so we can see why a loan was/wasn't approved and use our domain expertise to validate or correct the decision. => we need to find an approach to interpret model, that's how Model Interpretability is come up with.\n",
    "\n",
    "\n",
    "\n",
    "Part 1: Machine Learning Model Interpretability (this article) : https://gilberttanner.com/blog/introduction-to-machine-learning-model-interpretation/\n",
    "\n",
    "Key take away:\n",
    "What features are important ?\n",
    "- Feature importance : While feature importance shows us what features are important, it doesn't give us information on the effect of a particular change in the feature. A partial dependence plot can show whether the relationship between the target and a feature is linear, exponential or more complex.\n",
    "- Partial Dependence Plots (PDP): While feature importance shows us what features are important, it doesn't give us information on the effect of a particular change in the feature. A partial dependence plot can show whether the relationship between the target and a feature is linear, exponential or more complex.\n",
    "\n",
    "Understand individual predictions\n",
    "Why did the model make this specific prediction? This question becomes increasingly more important as machine learning models are increasingly used in applications like fraud detection or medical tasks because for these kinds of applications, it is imperative to validate and justify the results produced by a model.\n",
    "- LIME (LOCAL INTERPRETABLE MODEL-AGNOSTIC EXPLANATIONS) : In the paper \"Why Should I Trust You?\" the authors propose a method called Local interpretable model-agnostic explanations (LIME), where surrogate models are trained to approximate the predictions of the underlying black box model locally instead of globally.\n",
    "\n",
    "It achieves this by creating a new dataset from permuted data points around a data point of interest and the corresponding predictions of the black-box model. LIME then uses this new dataset to train an interpretable model like a tree or linear model, which then can be used to explain the black box model at this local point.\n",
    "- Shapley Values: In the case of machine learning, the \"game\" is the prediction task for a data point. The \"gain\" is the prediction minus the average prediction of all instances, and the \"players\" are the feature values of the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f5388",
   "metadata": {},
   "source": [
    "Part 2: What features are important : https://gilberttanner.com/blog/hands-on-global-model-interpretation/\n",
    "- ELI5 library. ELI5 allows users to visualize and debug various Machine Learning  Models. It also offers more than just feature importance, including library-specific features and a text-explainer.\n",
    "- Partial Dependence Plots: For creating partial dependence plots, we will use the PDPbox library. PDPbox provides us with a few different well-designed plots, including partial dependence plots for a single feature and partial dependence plots for multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c8164",
   "metadata": {},
   "source": [
    "Part 3: https://gilberttanner.com/blog/local-model-interpretation-an-introduction/\n",
    "\n",
    "How to implement LIME and Shapey Values\n",
    "\n",
    "In general, Lime is very good for getting a quick look at what the model is doing but has problems with consistency. On the other hand, Shapley value delivers a full explanation, making it a lot more exact than Lime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9945ab3",
   "metadata": {},
   "source": [
    "# Forcasting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ecbe4",
   "metadata": {},
   "source": [
    "Arima\n",
    "\n",
    "https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/#:~:text=time%20series%20forecasts.-,ARIMA%20is%20an%20acronym%20that%20stands%20for%20AutoRegressive%20Integrated%20Moving,aspects%20of%20the%20model%20itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5025d98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "18ce0ef201fdc62a779c25efabc25861319cfa824f17240b801462bbce9d1ec4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
